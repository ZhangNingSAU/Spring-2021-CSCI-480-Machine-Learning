{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning & Random Forests\n",
    "\n",
    "Suppose we ask a random question to thousands of people, then aggregate their answers. In many cases we will find that this aggregated answer is better than an **expert's answer** (*really*). This is called the *wisdom of the crowd*.\n",
    "\n",
    "Similarly, if we aggregate the predictions of a group of models (such as classifiers or regressors), we will often get better predictions than the best individual predictor.\n",
    "\n",
    "A group of predictors is called an ensemble. Thus this technique is called **ensemble learning**, and an ensemble learning algorithm is called an **Ensemble Method**.\n",
    "\n",
    "As an example of an ensemble method, we can train a group of decision tree classifiers, each on a *random subset* of the training data. Such an ensemble of decision trees is called a **random forest**. Despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "\n",
    "You will often use ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor.\n",
    "\n",
    "In this chapter, we will discuss the most famous ensemble learning methods, including: **Bagging, Boosting, & Stacking**.\n",
    "\n",
    "## Voting Classifiers\n",
    "\n",
    "Suppose we have trained a few classifiers, each achieving an 80% accuracy. A very simple way to create an even better classifiers is to aggregate the predictions of all our classifiers and choose the prediction that is the most frequent.\n",
    "\n",
    "Majority voting classification is called **Hard Voting**:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"../Resources/Hard_voting.png\"></div>\n",
    "\n",
    "Somewhat surprisingly, this classifier achieves an even better accuracy than the best predictor in the ensemble. Even if each classifier is a weak learner (does slightly better then random guessing). Assuming that we have a sufficient number of weak learners and enough diversity.\n",
    "\n",
    "Due to the law of large numbers, if we build an ensemble containing 1,000 classifiers with individual accuracies of $51%$ & trained for binary classification, If we predict the majority voting class, we can hope for up to $75%$ accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum {n \\choose k} p^{k} (1-p)^{(n-k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7260985557305037\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "p = 0.51\n",
    "total = 0\n",
    "# if we start from 501, then total probability is around 0.72\n",
    "for i in range(501,1001):\n",
    "    total = total + math.comb(1000,i)*p**i*(1-p)**(1000-i)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only true if all classifiers are completely independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data.\n",
    "\n",
    "One way to get diverse classifiers is use different algorithms for each one of them & train them on different subset of the training data. \n",
    "\n",
    "Let's implement a hard voting ensemble learner using scikit-learn:\n",
    "\n",
    "[VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs')\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=10000, noise=0.5,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6700, 2), (6700,), (3300, 2), (3300,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the performance of each classifier + ensemble method on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.803939393939394\n",
      "RandomForestClassifier 0.7963636363636364\n",
      "SVC 0.8215151515151515\n",
      "VotingClassifier 0.8178787878787879\n"
     ]
    }
   ],
   "source": [
    "for clf in [log_clf, rf_clf, svm_clf, voting_clf]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.803939393939394\n",
      "RandomForestClassifier 0.8006060606060607\n",
      "SVC 0.8215151515151515\n",
      "VotingClassifier 0.8221212121212121\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver='lbfgs')\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "# You need to set probability=True for SVC classifier, because default value is False\n",
    "svm_clf = SVC(gamma='scale',probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)],\n",
    "                              voting='soft')\n",
    "\n",
    "for clf in [log_clf, rf_clf, svm_clf, voting_clf]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_hat = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it! The `soft` voting classifier slightly outperforms the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ensemble method learners can estimate class probabilities, we can average their probabilities per class then predict the class with the highest probability. This is called **Soft voting**. It often yields results better than hard voting because it weights confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging & Pasting\n",
    "\n",
    "Another approach to having different algorithms trained on the same dataset is to have one algorithm but trained on random subsets of the training data. \n",
    "\n",
    "When subset sampling is performed with replacement, this is called **bagging**, and when sampling is performed without replacement, this is called **Pasting**.\n",
    "\n",
    "[Sampling with/without replacement](https://www.statisticshowto.com/sampling-with-replacement-without/)\n",
    "\n",
    "Bagging is showcased in the following figure:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"../Resources/bagging.png\"></div>\n",
    "\n",
    "Once the ensemble method is done training, we can infer using the *mode* for classification or *avg* for regression.\n",
    "\n",
    "Each individual predictor has a higher bias than if it were trained on the whole training dataset, but aggregation reduces both bias and variance.\n",
    "\n",
    "Because ensemble learners use separate different algorithms, they can be easily parallalizable, and that is why bagging and pasting are so popular, they scale very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging & Pasting in Scikit-Learn\n",
    "\n",
    "Let's implement bagging in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging models often result in a slightly more biased model than the base predictor, but because of the diversity of the training subsets, it has much smoother decision curves meaning it has usually a lower variance.\n",
    "\n",
    "The comparable/slightly higher bias comes from the fact that we are sampling with replacement, leading to points making it to multiple child learners.\n",
    "\n",
    "The following figure compares a normal decision tree and the same tree but using Bagging:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"../Resources/dt_bagging.png\"></div>\n",
    "\n",
    "Overall, bagging produces better models, and this explains why it's very popular. If we have enough compute time and power, we can also use cross validation to test both bagging and pasting for our specific project and decide which works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances will be sampled several times for several predictors, while others won't be sampled at all. Only about 63% of the instances are sampled for each predictor(see `max_samples=0.63` in the following cell). The other 37% of instances not sampled are called Out-of-Bag (oob) instances. Note that they're not the same 37% for all predictors.\n",
    "\n",
    "Since the individual predictor never sees the oob samples, it can be evaluated on the oob data without the need for a separate validation set.\n",
    "\n",
    "We can evaluate the ensemble itself by averaging the oob scores of each predictor.\n",
    "\n",
    "Let's do this using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=0.63, bootstrap=True, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=0.63,\n",
       "                  n_estimators=500, n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8067164179104478"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this estimation using the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = bag_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8078787878787879"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough!\n",
    "\n",
    "The oob decision function is also available using scikit-learn's API, since our classifier is a decision tree then the decision function is a ratio that can be expressed as a probability, let's take a look at it for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39711191, 0.60288809],\n",
       "       [0.18518519, 0.81481481],\n",
       "       [0.71161049, 0.28838951],\n",
       "       ...,\n",
       "       [0.06181818, 0.93818182],\n",
       "       [0.        , 1.        ],\n",
       "       [0.92913386, 0.07086614]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches & Random Subspaces\n",
    "\n",
    "We can sample features as well. Sampling is controlled by two hyper-parameters: `max_features` & `bootstrap_features`.\n",
    "\n",
    "Each predictor will be trained on a random feature sample. This technique is especially useful when you're dealing with high-dimensional input as as imagery. \n",
    "\n",
    "Sampling both instances and features is called \"random patches\" method. Sampling only features is called \"random subspaces\" method. Sampling features results in a more predictor diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A random forest is an ensemble of decision trees.\n",
    "\n",
    "Let's use its scikit-learn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rnd_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following `BaggingClassifier` is roughly equivalent to the previous `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees\n",
    "\n",
    "When we are growing a tree in a random forest, only a subset of features are considered when splitting. \n",
    "\n",
    "To make the trees more random, we can choose different thresholds of the features rather than searching for the best possible threshold. Such resulting model is called **Extremely Randomized Trees Ensemble**.\n",
    "\n",
    "We don't really know if an extremely randomized trees model will or will not outperform a classical random forest model. Generally, the only way to know is to try both and check their results using cross validation.\n",
    "\n",
    "See documentation for [sklearn.ensemble.ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [sklearn.ensemble.ExtraTreesRegresso](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Yet another great quality of random forests is that they make it so easy to measure the importance of each feature.\n",
    "\n",
    "We measure the importance by averaging the reduced impurity of each node that uses a certain feature across all trees in the forest. More exactly it's a weighted average beacuse each node has a number of samples that is used to weight.\n",
    "\n",
    "Let's get feature importance using random forests on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(iris['data'], iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09593054850777244\n",
      "sepal width (cm) 0.02549556416926862\n",
      "petal length (cm) 0.4267509724384687\n",
      "petal width (cm) 0.45182291488449017\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'a'), (2, 'b'), (3, 'c'))\n",
      "((1, 'a'), (2, 'b'), (3, 'c'))\n"
     ]
    }
   ],
   "source": [
    "t1 = (1, 2, 3)\n",
    "t2 = (\"a\", \"b\", \"c\")\n",
    "\n",
    "t = zip(t1, t2)\n",
    "\n",
    "#use the tuple() function to display a readable version of the result:\n",
    "\n",
    "print(tuple(t))\n",
    "\n",
    "\n",
    "t1 = (1, 2, 3)\n",
    "t2 = (\"a\", \"b\", \"c\", \"d\")\n",
    "\n",
    "t = zip(t1, t2)\n",
    "\n",
    "#use the tuple() function to display a readable version of the result:\n",
    "\n",
    "print(tuple(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot pixel-wise MNIST feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(digits['data'], digits['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rnd_clf.feature_importances_.reshape(8,8), cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest are very handy to get a quick understanding of what features actually matter, particularly if you need to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. \n",
    "\n",
    "There are many boosting methods available, but by far the most popular one is **AdaBoost** (Adaptive boosting) and **gradient boosting**.\n",
    "\n",
    "Let's start with Adaboost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that were underfitted. This results in the later predictors focusing more and more on the hard cases (the technique used by Adaboost).\n",
    "\n",
    "The following figure demenstrates the process\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"../Resources/adaboost.png\"></div>\n",
    "\n",
    "The algorithm first trains a base classifier and use it to make predictions on the training set. It then increases the weight of the missclassified instances and pass its weights + the data to the next predictor.\n",
    "\n",
    "This iterative learning approach shares similarities with gradient descent, except in gradient descent we're using the gradients to minimize a cost function, but with `AdaBoost` we're simply adding more learners to the chain. There is one important drawback of this method, It cannot be parallalized because each predictor needs the output of the previous predictor.\n",
    "\n",
    "Let's take a closer look at the Adaboost Algorithm:\n",
    "\n",
    "1. Each instance weight $w^{(i)}$ is initially set to $1/m$.\n",
    "2. A first predictor is trained, and its weighted error rate $r_1$ is computed on the training set using the general formula, where $\\hat{y}_j^{(i)}$ is the $j^{th}$ predictor's prediction for the $i^{th}$ instance.\n",
    "\n",
    "$$r_j=\\frac{\\sum_{i=1(\\hat{y_j^{(i)}}\\neq{y^{(i)}})}^{m} w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}$$\n",
    "\n",
    "3. the predictor's weight $\\alpha_{0}$ is computed as follows:\n",
    "\n",
    "$$\\alpha_{j}=\\eta log(\\frac{1-r_j}{r_j})$$\n",
    "\n",
    "The more accurate the predictor is, the higher its weight will be. If it's just guessing randomly, then its weight will be close to zero. However, If it's most often wrong, then its weight will be negative.\n",
    "\n",
    "4. The algorithm updates the instance weights which boosts the weights of the misclassified instances:\n",
    "\n",
    "$$\\forall i \\in \\{1,2, \\dots, m\\} \\\\ w^{(i)} \\leftarrow \\begin{cases}\n",
    "w^{(i)},  & \\text{if $\\hat{y}^{(i)}_j = y^{(i)}$} \\\\\n",
    "w^{(i)}exp(\\alpha_{j}), & \\text{else}\n",
    "\\end{cases}$$\n",
    "\n",
    "Then all the instance weights are normalized (are divided by the sum of the weights).\n",
    "\n",
    "5. Finally, a new predictor is trained using the updated weights, and the whole process is repeated again.\n",
    "\n",
    "The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found. To make predictions, Adaboost simply computes the predictions of all the predictors and weight them using the predictor weights $\\alpha_j$.\n",
    "\n",
    "The predicted class is the one that receives the majority of the weighted votes (where $N$ is the number of predictors):\n",
    "\n",
    "$$\\hat{y}(x)=argmax_{k}\\sum_{j=1 \\; \\hat{y}_j(x)=k}^{N}\\alpha_j$$\n",
    "\n",
    "Let's use scikit-learn's implementation of Adaboost with 1 depth decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                             n_estimators=200, algorithm='SAMME.R', \n",
    "                             learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ada_clf.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Just like Adaboost, Gradient Boosting works by sequentially adding predictors to an ensemble each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like Adaboost does, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.\n",
    "\n",
    "Let's go through a simple regression example, First let's fit a decision tree regressor to a synthetically generated training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(start=0, stop=1, num=500)\n",
    "y = (X-0.5)**2 + np.random.randn(500)/50.\n",
    "X = X[..., None]  # 1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.squeeze(), y.squeeze(), s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train a second decision tree regressor on the errors made by the first tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a third regressor on the errors made by the second regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.squeeze(), y.squeeze(), s=1)\n",
    "plt.scatter(X.squeeze(), y_pred.squeeze(), s=1, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure showcases the predictions made by sequential models, on the left we have gradient boosting, on the right we have Adaboost:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"../Resources/boosting_ensembles.png\"></div>\n",
    "\n",
    "A simpler way to train Gradient Boosting Regressor Trees ensemble is to use Scikit-Learn as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate in the hyper-parameters scales the contribution of each tree. If we set it to a low value, we will need more trees in the ensemble to fit the model to the training set, but the predictions will usually generalize better.\n",
    "\n",
    "For a fixed learning rate, we will have to figure out the optimal number of trees to not underfit/overfit. Early stopping can be used.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"../Resources/ensemble_overunderfit.png\"></div>\n",
    "\n",
    "The following code trains a `GBRT` ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "bst_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=2,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=74,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `argmin` based pick and the resulting model are showcased in the following figure:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 66%;\" src=\"../Resources/gradient_boosting_optimization.png\"></div>\n",
    "\n",
    "It is also possible to implement early stopping by actually stopping early and not export results for a large number of trees then use `argmin`. \n",
    "\n",
    "This code stops training when the validation error does not improve for five iterations in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that an optimized implementation of gradient boosting is available in the popular python library **`XGBoost`** (Stands for Extreme Gradient Boosting).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-e528dc1a32ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# you need to install xgboost \n",
    "# try `pip install xgboost` first\n",
    "# if any error, here is the installation guide\n",
    "# https://xgboost.readthedocs.io/en/latest/build.html\n",
    "# there is something wrong with xgboost on my laptop, I will update this jupyter notebook file later\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library can take care of early stopping for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:41] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[0]\tvalidation_0-rmse:0.386776\n",
      "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-rmse:0.349144\n",
      "[2]\tvalidation_0-rmse:0.31514\n",
      "[3]\tvalidation_0-rmse:0.284409\n",
      "[4]\tvalidation_0-rmse:0.256772\n",
      "[5]\tvalidation_0-rmse:0.23159\n",
      "[6]\tvalidation_0-rmse:0.209239\n",
      "[7]\tvalidation_0-rmse:0.188911\n",
      "[8]\tvalidation_0-rmse:0.170811\n",
      "[9]\tvalidation_0-rmse:0.154397\n",
      "[10]\tvalidation_0-rmse:0.139691\n",
      "[11]\tvalidation_0-rmse:0.126491\n",
      "[12]\tvalidation_0-rmse:0.114665\n",
      "[13]\tvalidation_0-rmse:0.103941\n",
      "[14]\tvalidation_0-rmse:0.094375\n",
      "[15]\tvalidation_0-rmse:0.085778\n",
      "[16]\tvalidation_0-rmse:0.07798\n",
      "[17]\tvalidation_0-rmse:0.071142\n",
      "[18]\tvalidation_0-rmse:0.065024\n",
      "[19]\tvalidation_0-rmse:0.059581\n",
      "[20]\tvalidation_0-rmse:0.05463\n",
      "[21]\tvalidation_0-rmse:0.050262\n",
      "[22]\tvalidation_0-rmse:0.046396\n",
      "[23]\tvalidation_0-rmse:0.043042\n",
      "[24]\tvalidation_0-rmse:0.040033\n",
      "[25]\tvalidation_0-rmse:0.037395\n",
      "[26]\tvalidation_0-rmse:0.035047\n",
      "[27]\tvalidation_0-rmse:0.033029\n",
      "[28]\tvalidation_0-rmse:0.031416\n",
      "[29]\tvalidation_0-rmse:0.029887\n",
      "[30]\tvalidation_0-rmse:0.02858\n",
      "[31]\tvalidation_0-rmse:0.027483\n",
      "[32]\tvalidation_0-rmse:0.026584\n",
      "[33]\tvalidation_0-rmse:0.025829\n",
      "[34]\tvalidation_0-rmse:0.025172\n",
      "[35]\tvalidation_0-rmse:0.024614\n",
      "[36]\tvalidation_0-rmse:0.02416\n",
      "[37]\tvalidation_0-rmse:0.023827\n",
      "[38]\tvalidation_0-rmse:0.023487\n",
      "[39]\tvalidation_0-rmse:0.023175\n",
      "[40]\tvalidation_0-rmse:0.022949\n",
      "[41]\tvalidation_0-rmse:0.022766\n",
      "[42]\tvalidation_0-rmse:0.022623\n",
      "[43]\tvalidation_0-rmse:0.022492\n",
      "[44]\tvalidation_0-rmse:0.022385\n",
      "[45]\tvalidation_0-rmse:0.022307\n",
      "[46]\tvalidation_0-rmse:0.022225\n",
      "[47]\tvalidation_0-rmse:0.02216\n",
      "[48]\tvalidation_0-rmse:0.022104\n",
      "[49]\tvalidation_0-rmse:0.022054\n",
      "[50]\tvalidation_0-rmse:0.022013\n",
      "[51]\tvalidation_0-rmse:0.021979\n",
      "[52]\tvalidation_0-rmse:0.021945\n",
      "[53]\tvalidation_0-rmse:0.021928\n",
      "[54]\tvalidation_0-rmse:0.021908\n",
      "[55]\tvalidation_0-rmse:0.021899\n",
      "[56]\tvalidation_0-rmse:0.021891\n",
      "[57]\tvalidation_0-rmse:0.02189\n",
      "[58]\tvalidation_0-rmse:0.021876\n",
      "[59]\tvalidation_0-rmse:0.021867\n",
      "[60]\tvalidation_0-rmse:0.021846\n",
      "[61]\tvalidation_0-rmse:0.021857\n",
      "[62]\tvalidation_0-rmse:0.021863\n",
      "Stopping. Best iteration:\n",
      "[60]\tvalidation_0-rmse:0.021846\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004772392356123875"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_val.squeeze(), y_val.squeeze(), s=1)\n",
    "plt.scatter(X_val.squeeze(), y_pred.squeeze(), s=1, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "The last ensemble method we will discuss in this chapter is called stacking (short for stacked generalization).\n",
    "It is based on a simple idea: Instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, Why don't we train a model to do that for us? \n",
    "\n",
    "The final aggregator learner is typically called a **blender** or a **meta-learner**.\n",
    "\n",
    "The following figure outlines the idea of learning aggregations:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width: 50%;\" src=\"../Resources/blender.png\"></div>\n",
    "\n",
    "To train a blender, a common approach is to use a holdout set.\n",
    "\n",
    "We first train predictors on the first holdout set. Then the first layer predictors are used to export predictions of the second held-out set. This ensures that the predictions are clean since the predictors never saw the 2nd held-out instances during training.\n",
    "\n",
    "The result is that for each instance in the holdout set, we have 3 predictions, the 3 predictions per row will be considered as input for the blender. The blender is trained on this new dataset.\n",
    "\n",
    "Unfortunately, `scikit-learn` doesn't support stacking directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, How? If not, Why?**\n",
    "\n",
    "Yes, we can combine their predictions:\n",
    "- Classification Problems\n",
    "    - hard voting (`y_hat=mode(preds)`)\n",
    "    - soft voting for classification (averaging estimated class probabilities and taking the class associated with the highest one).\n",
    "- Regression Problems\n",
    "    - Taking the mean of the predicted values from each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the difference between hard & soft voting classifiers?**\n",
    "In Hard voting, we take all of the predictions and output the mode (the most frequently predicted class). In Soft voting, we average all probabilities associated with each class and output the class with the highest averaged probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles? Boosting ensembles, random forests, stacking ensembles?**\n",
    "\n",
    "- Bagging Ensemble (Bootstrap Aggregation  Sampling with replacement + feature subsetting): It is parallelizable.\n",
    "- Pasting Ensemble (Only difference  Sampling without replacement): It is parallelizable.\n",
    "- Boosting Ensemble (Trains predictors sequentially): Not parallelizable.\n",
    "- Random Forests: Parallelizable.\n",
    "- Stacking Ensemble: Stacking is partly parallelizable, we can train massive predictors in the first layer but we have to wait for the ith training phase to finish before training subsequent layers (and the final blender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What is the benefit of out-of-bag evaluation?**\n",
    "\n",
    "If we use subsets from the training dataset while training, we can evaluate each individual predictor on the unsampled data and estimate the overall performance of the ensemble model without having a hold-out validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What makes Extra-Trees more random than regular random forests? How can this extra randomness help? Are extra trees slower or faster than regular random forests?**\n",
    "\n",
    "Extra-Trees are more random than regular random forests in that they choose a subset of features to select from at each split and they don't necessary choose the best threshold but they take random thresholds.\n",
    "\n",
    "This added randomness helps in creating much more varied trees and helps in training a good model.\n",
    "\n",
    "Extra-trees are faster then regular random forests because they don't have to check for the best split on all features and thresholds, but extra-trees aren't faster then random forests in inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. If your Adaboost ensemble underfits the training data, which hyper-parameters should you tweak and how?**\n",
    "\n",
    "We should increase the number of estimators and reduce the regularization hyper-parameters for the base estimator.\n",
    "\n",
    "We can also increase the learning rate.\n",
    "\n",
    "Finally, we can try decreasing $\\eta$ so that weights are not extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. If your gradient boosting ensemble overfits the training set, should you increase or decrease the learning rate?** \n",
    "\n",
    "We should decrease the learning rate because it shrinks the contribution of each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load MNIST data, and split it into a training set, a validation set, and a test set (Use 50k instances for training, 10k for validation, and 10k for testing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.fetch_openml(name='mnist_784', return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle and sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.permutation(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[random_indices]\n",
    "y = y[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that it's fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[33].reshape(28, 28), cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = X[:50000], y[:50000]\n",
    "X_val, y_val = X[50000:60000], y[50000:60000]\n",
    "X_test, y_test = X[60000:], y[60000:]\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have enough compute power, let's take 10% of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 784), (5000,), (1000, 784), (1000,), (1000, 784), (1000,))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = X_train[:5000], y_train[:5000]\n",
    "X_val, y_val = X_val[:1000], y_val[:1000]\n",
    "X_test, y_test = X_test[:1000], y_test[:1000]\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a Random Forests Classifier, an Extra-Trees classifier, & an SVM classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score as acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "etc = ExtraTreesClassifier(n_estimators=10)\n",
    "svc = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "                     oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedakramzaytar/opt/miniconda3/envs/research/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.892, 0.882, 0.096)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_val, rfc.predict(X_val)), acc(y_val, etc.predict(X_val)), acc(y_val, svc.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_voter = VotingClassifier(estimators=[('random forest', rfc), \n",
    "                                          ('extra trees', etc), \n",
    "                                          ('support vector classifier', svc)], \n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random forest',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=10,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=No...\n",
       "                                                   random_state=None, verbose=0,\n",
       "                                                   warm_start=False)),\n",
       "                             ('support vector classifier',\n",
       "                              SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma='auto_deprecated',\n",
       "                                  kernel='rbf', max_iter=-1, probability=True,\n",
       "                                  random_state=None, shrinking=True, tol=0.001,\n",
       "                                  verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=-1, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_voter.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_voter = VotingClassifier(estimators=[('random forest', rfc), \n",
    "                                          ('extra trees', etc), \n",
    "                                          ('support vector classifier', svc)],\n",
    "                              voting='soft',\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random forest',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=10,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=No...\n",
       "                                                   random_state=None, verbose=0,\n",
       "                                                   warm_start=False)),\n",
       "                             ('support vector classifier',\n",
       "                              SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decision_function_shape='ovr',\n",
       "                                  degree=3, gamma='auto_deprecated',\n",
       "                                  kernel='rbf', max_iter=-1, probability=True,\n",
       "                                  random_state=None, shrinking=True, tol=0.001,\n",
       "                                  verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_voter.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_val, hard_voter.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.919"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_val, soft_voter.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.88, 0.882, 0.103)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_test, rfc.predict(X_test)), acc(y_val, etc.predict(X_val)), acc(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_test, hard_voter.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.909"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_test, soft_voter.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Run the previous classifiers from the previous exercice to make predictions on the validation set, and create a new training set with the resulting predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1), (1000, 1), (1000, 1))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc_preds = etc.predict(X_val)[..., None]\n",
    "rfc_preds = rfc.predict(X_val)[..., None]\n",
    "svc_preds = svc.predict(X_val)[..., None]\n",
    "etc_preds.shape, rfc_preds.shape, svc_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the class image. Train a classifier on this new training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_ = np.concatenate((etc_preds, rfc_preds, etc_preds), axis=1)\n",
    "X_val_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_.fit(X_val_, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now evaluate the ensemble on the test set.**\n",
    "1. For each image in the test set, make predictions with all your classifiers\n",
    "2. feed the predictions to the blender to get the ensembles predictions.\n",
    "3. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc_preds = etc.predict(X_test)[..., None]\n",
    "rfc_preds = rfc.predict(X_test)[..., None]\n",
    "svc_preds = svc.predict(X_test)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = np.concatenate((etc_preds, rfc_preds, etc_preds), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = rfc_.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.887"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y_test, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does better than the hard voter and worse then the softvoter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
